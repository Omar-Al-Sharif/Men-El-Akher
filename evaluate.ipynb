{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "\n",
    "text = \"شهدت مدينة طرابلس، مساء أمس الأربعاء، احتجاجات شعبية وأعمال شغب لليوم الثالث على التوالي، وذلك بسبب تردي الوضع المعيشي والاقتصادي. واندلعت مواجهات عنيفة وعمليات كر وفر ما بين الجيش اللبناني والمحتجين استمرت لساعات، إثر محاولة فتح الطرقات المقطوعة، ما أدى إلى إصابة العشرات من الطرفين\"\n",
    "\n",
    "model_name = \"aubmindlab/bert-base-arabertv2\"\n",
    "preprocessor = ArabertPreprocessor(model_name=model_name)\n",
    "\n",
    "def preprocess_function(text):\n",
    "  return preprocessor.preprocess(text)\n",
    "\n",
    "def unpreprocess_function(text):\n",
    "  temp = preprocessor.unpreprocess(text)\n",
    "  temp = re.sub('\\[CLS\\]|\\[PAD\\]|\\[SEP\\]', '', temp)\n",
    "  return re.sub('\\s+$|^\\s', '', temp)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./AraT5\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./finetuned-model\")\n",
    "\n",
    "\n",
    "# def tokenize_function(examples):\n",
    "#   inputs = [preprocess_function(text) for text in examples['text']]\n",
    "#   inputs = examples['text']\n",
    "#   model_inputs = tokenizer(inputs, padding='max_length', truncation=True)\n",
    "\n",
    "#   labels = tokenizer(examples['summary'], padding='max_length', max_length=128, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "#   model_inputs['labels'] = labels['input_ids']\n",
    "#   return model_inputs\n",
    "\n",
    "inputs = tokenizer(preprocess_function(text), return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)\n",
    "decoded_text=tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "unpreprocessed_text=unpreprocess_function(decoded_text)\n",
    "print(unpreprocessed_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
